
// Cargar el conjunto de datos
// Cargar el conjunto de datos
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']
auto_mpg_data = pd.read_csv(url, names=column_names, delim_whitespace=True)

// Mostrar las primeras filas del conjunto de datos
// Mostrar las primeras filas del conjunto de datos
print(auto_mpg_data.head())
// Reemplazar los valores "?" en la columna "horsepower" con NaN
// Reemplazar los valores "?" en la columna "horsepower" con NaN
auto_mpg_data['horsepower'].replace('?', pd.NA, inplace=True)

// Convertir la columna "horsepower" a tipo numérico
// Convertir la columna "horsepower" a tipo numérico
auto_mpg_data['horsepower'] = pd.to_numeric(auto_mpg_data['horsepower'], errors='coerce')

// Rellenar los valores faltantes con la media de la columna "horsepower"
// Rellenar los valores faltantes con la media de la columna "horsepower"
auto_mpg_data['horsepower'].fillna(auto_mpg_data['horsepower'].mean(), inplace=True)

// Mostrar las primeras filas del conjunto de datos después de manejar los valores faltantes
// Mostrar las primeras filas del conjunto de datos después de manejar los valores faltantes
print(auto_mpg_data.head())
// Paso 3: Convertir las variables categóricas en variables dummy si es necesario
// Verificar si hay columnas categóricas
// Paso 3: Convertir las variables categóricas en variables dummy si es necesario
// Verificar si hay columnas categóricas
categorical_cols = auto_mpg_data.select_dtypes(include=['object']).columns.tolist()

if categorical_cols:
    // Convertir las columnas categóricas en variables dummy
    // Convertir las columnas categóricas en variables dummy
    auto_mpg_data = pd.get_dummies(auto_mpg_data, columns=categorical_cols)

// Mostrar las primeras filas del conjunto de datos después de convertir las variables categóricas
// Mostrar las primeras filas del conjunto de datos después de convertir las variables categóricas
print("\nConjunto de datos después de convertir las variables categóricas en variables dummy:")
print(auto_mpg_data.head())
from sklearn.preprocessing import StandardScaler

// Paso 4: Normalizar las características para mejorar la convergencia del modelo
// Seleccionar las características para la normalización (todas excepto la columna objetivo 'mpg')
// Paso 4: Normalizar las características para mejorar la convergencia del modelo
// Seleccionar las características para la normalización (todas excepto la columna objetivo 'mpg')
features = auto_mpg_data.drop(columns=['mpg'])

// Inicializar el transformador de escalado
// Inicializar el transformador de escalado
scaler = StandardScaler()

// Normalizar las características seleccionadas
// Normalizar las características seleccionadas
scaled_features = scaler.fit_transform(features)

// Crear un nuevo DataFrame con las características normalizadas
// Crear un nuevo DataFrame con las características normalizadas
auto_mpg_normalized = pd.DataFrame(scaled_features, columns=features.columns)

// Agregar la columna objetivo 'mpg' al DataFrame normalizado
// Agregar la columna objetivo 'mpg' al DataFrame normalizado
auto_mpg_normalized['mpg'] = auto_mpg_data['mpg']

// Mostrar las primeras filas del conjunto de datos normalizado
// Mostrar las primeras filas del conjunto de datos normalizado
print("\nConjunto de datos después de normalizar las características:")
print(auto_mpg_normalized.head())
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

// Paso 5: Construcción del modelo de red neuronal
// Definir el modelo secuencial
// Paso 5: Construcción del modelo de red neuronal
// Definir el modelo secuencial
model = Sequential()

// Añadir capas ocultas
// Añadir capas ocultas
model.add(Dense(64, activation='relu', input_shape=(len(auto_mpg_normalized.columns) - 1,)))
model.add(Dense(32, activation='relu'))

// Añadir capa de salida
// Añadir capa de salida
model.add(Dense(1, activation='linear'))

// Compilar el modelo
// Compilar el modelo
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])

// Mostrar un resumen del modelo
// Mostrar un resumen del modelo
print("\nResumen del modelo:")
model.summary()
from sklearn.model_selection import train_test_split

// Paso 6: Entrenamiento del modelo
// Dividir los datos en conjuntos de entrenamiento y prueba
// Paso 6: Entrenamiento del modelo
// Dividir los datos en conjuntos de entrenamiento y prueba
X = auto_mpg_normalized.drop(columns=['mpg'])
y = auto_mpg_normalized['mpg']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

// Entrenar el modelo
// Entrenar el modelo
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)
// Paso 7: Evaluación del modelo
// Evaluar el rendimiento del modelo utilizando los datos de prueba
// Paso 7: Evaluación del modelo
// Evaluar el rendimiento del modelo utilizando los datos de prueba
loss, mse = model.evaluate(X_test, y_test)

// Mostrar la pérdida y el error cuadrático medio (MSE)
// Mostrar la pérdida y el error cuadrático medio (MSE)
print("\nPérdida en los datos de prueba:", loss)
print("Error cuadrático medio en los datos de prueba:", mse)
from tensorflow.keras.layers import Dropout

// Optimización del modelo
// Definir el modelo secuencial con capa de Dropout
// Optimización del modelo
// Definir el modelo secuencial con capa de Dropout
model_optimized = Sequential()

// Añadir capas ocultas con Dropout
// Añadir capas ocultas con Dropout
model_optimized.add(Dense(64, activation='relu', input_shape=(len(auto_mpg_normalized.columns) - 1,)))
model_optimized.add(Dropout(0.2))  # Añadir Dropout con una tasa de 0.2 para prevenir el sobreajuste
model_optimized.add(Dense(32, activation='relu'))
model_optimized.add(Dropout(0.2))

// Añadir capa de salida
// Añadir capa de salida
model_optimized.add(Dense(1, activation='linear'))

// Compilar el modelo optimizado
// Compilar el modelo optimizado
model_optimized.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])

// Entrenar el modelo optimizado
// Entrenar el modelo optimizado
history_optimized = model_optimized.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)
// Evaluación del modelo optimizado
loss_optimized, mse_optimized = model_optimized.evaluate(X_test, y_test)

// Mostrar la pérdida y el error cuadrático medio (MSE) del modelo optimizado
print("\nPérdida en los datos de prueba (modelo optimizado):", loss_optimized)
print("Error cuadrático medio en los datos de prueba (modelo optimizado):", mse_optimized)
// Evaluación del modelo optimizado
loss_optimized, mse_optimized = model_optimized.evaluate(X_test, y_test)

// Mostrar la pérdida y el error cuadrático medio (MSE) del modelo optimizado
print("\nPérdida en los datos de prueba (modelo optimizado):", loss_optimized)
print("Error cuadrático medio en los datos de prueba (modelo optimizado):", mse_optimized)
